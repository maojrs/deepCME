{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "concA = 1\n",
    "concB = 2\n",
    "k1 = 3.0\n",
    "k2 = 0.6\n",
    "k3 = 0.25\n",
    "k4 = 2.95\n",
    "vol = 30 #100.0\n",
    "\n",
    "concA = 10\n",
    "concB = 20\n",
    "k1 = 6\n",
    "k2 = 1.0\n",
    "k3 = 230\n",
    "k4 = 1000\n",
    "vol = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CME birth/death rates\n",
    "def lambdan(n):\n",
    "    return concA*k1*n*(n-1)/vol + concB*k3*vol\n",
    "def mun(n):\n",
    "    return k2*n*(n-1)*(n-2)/vol**2 + n*k4\n",
    "\n",
    "# Define intensity functions for tau-leaping\n",
    "def lambda1(n):\n",
    "    return concA*k1*n*(n-1)/vol\n",
    "def lambda2(n):\n",
    "    return k2*n*(n-1)*(n-2)/vol**2\n",
    "def lambda3(n):\n",
    "    return concB*k3*vol\n",
    "def lambda4(n):\n",
    "    return  n*k4\n",
    "\n",
    "# Define terminal condition function\n",
    "def terminalCondition(n):\n",
    "    return 1.0*n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do tau-leap integration loop\n",
    "def target_func(X0):\n",
    "    N = np.zeros([timesteps, len(X0)])\n",
    "    for j in range(len(X0)):\n",
    "        N[0,j] = X0[j]\n",
    "    for i in range(timesteps-1):\n",
    "        for j in range(len(X0)):\n",
    "            numReactions1 = np.random.poisson(lambda1(N[i,j])*dt, 1)\n",
    "            numReactions2 = np.random.poisson(lambda2(N[i,j])*dt, 1)\n",
    "            numReactions3 = np.random.poisson(lambda3(N[i,j])*dt, 1)\n",
    "            numReactions4 = np.random.poisson(lambda4(N[i,j])*dt, 1)\n",
    "            N[i+1,j] = N[i,j] + numReactions1 - numReactions2 + numReactions3 - numReactions4\n",
    "            if N[i+1,j] < 0:\n",
    "                N[i+1,j] = 0\n",
    "            \n",
    "    return terminalCondition(N[-1,:])\n",
    "# Note one advantage is to calculate several terminal conditions at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation parameters\n",
    "timesteps = 10000\n",
    "dt = 0.0001\n",
    "\n",
    "# Network parameters\n",
    "DOMAIN = [10, 200]   \n",
    "\n",
    "# network settings\n",
    "WIDTH = 10 #35 #20              # number of neurons per hidden layer\n",
    "DEPTH = 8 #10 #6               # number of hidden layers\n",
    "ACTIVATION = tf.nn.relu # hidden layer activation function\n",
    "\n",
    "# training settings\n",
    "INIT_L_RATE  = 1e-3 #2e-1\n",
    "FINAL_L_RATE = 1e-5 #1e-3\n",
    "NUM_ITER     = 4000 #8000\n",
    "BATCH_SIZE   = 1024 #2048\n",
    "\n",
    "# Other\n",
    "OUTPUT_RES = 50\n",
    "datasize = 10*BATCH_SIZE\n",
    "FILENAME = \"data/schlogl_data_v\" + str(vol) + \"_\" + str(datasize) + \".txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate network structure\n",
    "\n",
    "# Inputs and targets placeholders for trained data\n",
    "networkInput = tf.placeholder(dtype=tf.float32, shape=(None,1), name='input')\n",
    "networkTarget = tf.placeholder(dtype=tf.float32, shape=(None,1), name='target')\n",
    "\n",
    "# Hidden layers\n",
    "hidden = (DEPTH-1)*[None]\n",
    "hidden[0] = networkInput\n",
    "#next_layer = networkInput\n",
    "for l in range(DEPTH-2):\n",
    "    #hidden[l+1]= tf.layers.dense(next_layer, WIDTH, activation=ACTIVATION)\n",
    "    #next_layer = tf.concat( hidden[:l+1], axis=1)\n",
    "    \n",
    "    #without skip connections\n",
    "    hidden[l+1] = tf.layers.dense(hidden[l], WIDTH, activation=ACTIVATION)\n",
    "    \n",
    "# Add predition outermost layer\n",
    "networkPrediction = tf.layers.dense(hidden[DEPTH-2], 1, activation=None, name='prediction')\n",
    "# With skip connections\n",
    "#networkPrediction = tf.layers.dense(tf.concat(hidden, axis=1), 1, activation=None, name='prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training procedure\n",
    "\n",
    "# Define loss function and error\n",
    "loss = tf.reduce_mean(tf.square(networkPrediction - networkTarget))\n",
    "error = tf.reduce_max(tf.abs(networkPrediction - networkTarget))\n",
    "\n",
    "# use stochastic gradient descent with ADAM during optimization\n",
    "step = tf.train.AdamOptimizer(INIT_L_RATE).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all parameters\n",
    "print('\\n----------------------------------------------------') \n",
    "print(' RUNNING EXPERIMENT WITH THE FOLLOWING PARAMETERS: ')\n",
    "print('----------------------------------------------------\\n')\n",
    "print('depth:\\t\\t\\t{}'.format(DEPTH))\n",
    "print('width:\\t\\t\\t{}'.format(WIDTH))\n",
    "print('number of neurons:\\t{}'.format(2+(DEPTH-2)*WIDTH))\n",
    "print('number of connections:\\t{}'.format(1+(DEPTH-2)*WIDTH*2+WIDTH*WIDTH*(DEPTH-3)*(DEPTH-2)//2))\n",
    "print('activation:\\t\\t{}'.format(ACTIVATION.__name__))\n",
    "print('learning rate:\\t\\t{} to {}'.format(INIT_L_RATE, FINAL_L_RATE))\n",
    "print('iterations:\\t\\t{}'.format(NUM_ITER))\n",
    "print('batch size:\\t\\t{}'.format(BATCH_SIZE))\n",
    "print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Input/target data in parallel\n",
    "generatedata = False\n",
    "\n",
    "# Do tau-leap integration loop (only for sclar initial condition)\n",
    "def target_func_one(X0):\n",
    "    N = np.zeros(timesteps)\n",
    "    N[0] = X0\n",
    "    for i in range(timesteps-1):\n",
    "        numReactions1 = np.random.poisson(lambda1(N[i])*dt, 1)\n",
    "        numReactions2 = np.random.poisson(lambda2(N[i])*dt, 1)\n",
    "        numReactions3 = np.random.poisson(lambda3(N[i])*dt, 1)\n",
    "        numReactions4 = np.random.poisson(lambda4(N[i])*dt, 1)\n",
    "        N[i+1] = N[i] + numReactions1 - numReactions2 + numReactions3 - numReactions4\n",
    "        if N[i+1] < 0:\n",
    "            N[i+1] = 0\n",
    "    return N\n",
    "\n",
    "\n",
    "if generatedata:\n",
    "    stride = 10\n",
    "    x0 = np.float32(np.random.randint( DOMAIN[0], DOMAIN[1], datasize ))\n",
    "    def propagate(x):\n",
    "        xt = np.float32(target_func_one(x))\n",
    "        y0 = np.zeros(int(timesteps/stride))\n",
    "        for j in range(int(timesteps/stride)):\n",
    "            y0[j] = 1.0*xt[j*stride]\n",
    "        return y0\n",
    "    \n",
    "    num_cores = multiprocessing.cpu_count() \n",
    "    results = Parallel(n_jobs=num_cores, verbose = 2)(delayed(propagate)(i) for i in x0)\n",
    "    print(\"Writing to file ...\", end=\"\\r\")\n",
    "    f = open(FILENAME,\"w\")\n",
    "    for i in range(len(results)):\n",
    "        f.write(\" \".join(str(x) for x in results[i]) + \"\\n\")\n",
    "    f.close()\n",
    "    print(\"Percentage finished:\", 100, \"%    \", end=\"\\r\")\n",
    "    \n",
    "# OLD NON-PARALLEL VERSION\n",
    "# if generatedata:\n",
    "#     timesteps = 10000\n",
    "#     dt = 0.1\n",
    "#     datasize = 50000\n",
    "#     stride = 10\n",
    "#     f = open(FILENAME,\"w\")\n",
    "#     for i in range(datasize):\n",
    "#         x0 = np.float32(np.random.randint( DOMAIN[0], DOMAIN[1] ))\n",
    "#         y0 = np.float32(target_func_one(x0))\n",
    "#         #f.write(str(x0) + ' ')\n",
    "#         for j in range(int(timesAteps/stride)):\n",
    "#             f.write(str(y0[j*stride]) + ' ')\n",
    "#         f.write(\"\\n\")\n",
    "#         #f.write(str(x0) + ' ' + str(y0) + \"\\n\")\n",
    "#         if i%100 == 0:\n",
    "#             print(\"Percentage finished:\", 100.0*float(i)/datasize, \"%    \", end=\"\\r\")\n",
    "#     f.close()\n",
    "#     print(\"Percentage finished:\", 100, \"%    \", end=\"\\r\")\n",
    "\n",
    "# else:\n",
    "#     timesteps = 1000\n",
    "#     dt = 0.1\n",
    "#     datasize = 8000\n",
    "#     inputData = (np.reshape(np.random.randint( DOMAIN[0], DOMAIN[1], datasize ) , [-1, 1])).astype('float32')\n",
    "#     targetData = (np.reshape( target_func(inputData), [-1, 1])).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readGenerateddata = True\n",
    "if readGenerateddata:\n",
    "    data = np.genfromtxt(FILENAME, delimiter=' ')\n",
    "    inputData = data[:,0]\n",
    "    targetData = terminalCondition(data[:,-1])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetData = terminalCondition(data[:,-1])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Tensorflow session and initialize all network variables\n",
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training\n",
    "\n",
    "# run gradient descent steps with Adam\n",
    "print('\\nStarted training...')\n",
    "print('{:8s}\\t{:8s}\\t{:8s}'.format('iter', 'l2-loss', 'linf-err'))\n",
    "print('{:8s}\\t{:8s}\\t{:8s}'.format(*(3*[8*'-'])))\n",
    "for iter in range(NUM_ITER):\n",
    "    # generate random batch of inputs and corresponding target values\n",
    "    #indices = (np.linspace(0,BATCH_SIZE-1,BATCH_SIZE)).astype(int)\n",
    "    indices =  np.reshape(np.random.randint(0, datasize, BATCH_SIZE), [-1,1])\n",
    "    inputBatch = np.take( inputData, indices)\n",
    "    targetBatch = np.take( targetData, indices)\n",
    "\n",
    "    # take gradient descent step and compute loss & error\n",
    "    loss_val, error_val, _ = session.run(\n",
    "        [loss, error, step],\n",
    "        feed_dict={networkInput: inputBatch, networkTarget: targetBatch}\n",
    "    )\n",
    "    if iter % 100 == 0:\n",
    "        print('{:8d}\\t{:1.2e}\\t{:1.2e}'.format(iter, loss_val, error_val))\n",
    "print('...finished training.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for scalar case\n",
    "plt.rcParams['figure.figsize'] = (12,9)\n",
    "# generate full sample grid of input domain\n",
    "RESOLUTION = 200\n",
    "xgrid = np.linspace(DOMAIN[0], DOMAIN[1], num=RESOLUTION)\n",
    "xgrid = xgrid.astype(int)\n",
    "input_test_batch = np.reshape(xgrid , [-1,1])\n",
    "\n",
    "# get model predictions\n",
    "prediction_test_batch = session.run( networkPrediction, feed_dict={networkInput: input_test_batch})\n",
    "\n",
    "# get actual target values and compare with predictions\n",
    "#target_test_batch = target_func(xgrid, ygrid)\n",
    "#l2_err = 1/2*np.mean(np.square(prediction_test_batch-target_test_batch))\n",
    "#linf_err = np.max(np.abs(prediction_test_batch-target_test_batch))\n",
    "#print(\n",
    "#    'Error of predictions after training, evaluated on {}x{} grid:'\n",
    "#    .format(RESOLUTION, RESOLUTION)\n",
    "#)\n",
    "#print('l2:\\t{:1.4e}'.format(l2_err))\n",
    "#print('l2inf:\\t{:1.4e}'.format(linf_err))\n",
    "\n",
    "# plot actual target, prediction, and comparison\n",
    "plt.plot(inputData, targetData, '.', label = \"Data\")\n",
    "plt.plot(input_test_batch, prediction_test_batch, '-r', linewidth=5, label = \"NN\")\n",
    "plt.xlim([5,200])\n",
    "plt.ylim([-20,200])\n",
    "plt.xlabel('Initial value $X[0]$', fontsize = 20)\n",
    "plt.ylabel('$X[T]$', fontsize = 20)\n",
    "#plt.title('target function')\n",
    "plt.legend(fontsize = 20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numReactions1 = np.random.poisson(lambda1(20)*dt, 1)\n",
    "numReactions2 = np.random.poisson(lambda2(20)*dt, 1)\n",
    "numReactions3 = np.random.poisson(lambda3(20)*dt, 1)\n",
    "numReactions4 = np.random.poisson(lambda4(20)*dt, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(numReactions1, numReactions2, numReactions3, numReactions4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
